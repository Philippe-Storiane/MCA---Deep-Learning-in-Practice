{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of CNN: Grad-CAM\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we will introduce the Grad-CAM which visualizes the heatmap of input images by highlighting the important region for visual question answering(VQA) task.\n",
    "\n",
    "* **To be submitted**: this notebook in two weeks, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Question Answering problem\n",
    "Given an image and a question in natural language, the model choose the most likely answer from 3 000 classes according to the content of image. The VQA task is indeed a multi-classificaition problem.\n",
    "<img src=\"vqa_model.PNG\">\n",
    "\n",
    "We provide you a pretrained model `vqa_resnet` for VQA tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from load_model import load_model\n",
    "vqa_resnet = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(vqa_resnet) # for more information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = '2017-08-04_00.55.19.pth'\n",
    "saved_state = torch.load(checkpoint, map_location=device)\n",
    "# reading vocabulary from saved model\n",
    "vocab = saved_state['vocab']\n",
    "\n",
    "# reading word tokens from saved model\n",
    "token_to_index = vocab['question']\n",
    "\n",
    "# reading answers from saved model\n",
    "answer_to_index = vocab['answer']\n",
    "\n",
    "num_tokens = len(token_to_index) + 1\n",
    "\n",
    "# reading answer classes from the vocabulary\n",
    "answer_words = ['unk'] * len(answer_to_index)\n",
    "for w, idx in answer_to_index.items():\n",
    "    answer_words[idx]=w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs\n",
    "In order to use the pretrained model, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(448, 448)`. You can call the function `image_to_features` to achieve image preprocessing. For input question, the function `encode_question` is provided to encode the question into a vector of indices. You can also use `preprocess` function for both image and question preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(target_size, central_fraction=1.0):\n",
    "    return transforms.Compose([\n",
    "        transforms.Scale(int(target_size / central_fraction)),\n",
    "        transforms.CenterCrop(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(question):\n",
    "    \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
    "    question_arr = question.lower().split()\n",
    "    vec = torch.zeros(len(question_arr), device=device).long()\n",
    "    for i, token in enumerate(question_arr):\n",
    "        index = token_to_index.get(token, 0)\n",
    "        vec[i] = index\n",
    "    return vec, torch.tensor(len(question_arr), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess requires the dir_path of an image and the associated question. \n",
    "#It returns the spectific input form which can be used directly by vqa model. \n",
    "def preprocess(dir_path, question):\n",
    "    q, q_len = encode_question(question)\n",
    "    img = Image.open(dir_path).convert('RGB')\n",
    "    image_size = 448  # scale image to given size and center\n",
    "    central_fraction = 1.0\n",
    "    transform = get_transform(image_size, central_fraction=central_fraction)\n",
    "    img_transformed = transform(img)\n",
    "    img_features = img_transformed.unsqueeze(0).to(device)\n",
    "    \n",
    "    inputs = (img_features, q.unsqueeze(0), q_len.unsqueeze(0))\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you two pictures and some question-answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question1 = 'What animal'\n",
    "Answer1 = ['dog','cat' ]\n",
    "indices1 = [answer_to_index[ans] for ans in Answer1]# The indices of category \n",
    "img1 = Image.open('dog_cat.png')\n",
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'dog_cat.png' \n",
    "inputs = preprocess(dir_path, Question1)\n",
    "ans = vqa_resnet(*inputs) # use model to predict the answer\n",
    "answer_idx = np.argmax(F.softmax(ans, dim=1).data.numpy())\n",
    "print(answer_words[answer_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question2 = 'What color'\n",
    "Answer2 = ['green','yellow' ]\n",
    "indices2 = [answer_to_index[ans] for ans in Answer2]\n",
    "img2 = Image.open('hydrant.png')\n",
    "print(img2.size)\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'hydrant.png' \n",
    "inputs = preprocess(dir_path, Question2)\n",
    "ans = vqa_resnet(*inputs) # use model to predict the answer\n",
    "answer_idx = np.argmax(F.softmax(ans, dim=1).data.numpy())\n",
    "print(answer_words[answer_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM \n",
    "* **Overview:** Given an image with a question, and a category (‘dog’) as input, we foward propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (dog), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the two images. For each image, consider the answers we provided as the desired classes. Compare the heatmaps of different answers, and conclude. \n",
    "\n",
    "\n",
    "* **Hints**: \n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully. \n",
    " + The pretrained model `vqa_resnet` doesn't have the activation function after its last layer, the output is indeed the `raw class scores`, you can use it directly. Run \"print(vqa_resnet)\" to get more information on VGG model.\n",
    " + The last CNN layer of the model is: `vqa_resnet.resnet_layer4.r_model.layer4[2].conv3` \n",
    " + The size of feature maps is 14x14, so as your heatmap. You need to project the heatmap to the original image(224x224) to have a better observation. The function `cv2.resize()` may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"grad_cam.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Cam(dir_path, question):\n",
    "    \n",
    "    features = None\n",
    "    grads = None\n",
    "    \n",
    "    def forward_hook(self, inpout, output):\n",
    "        nonlocal features\n",
    "        #print('Inside ' + self.__class__.__name__ + ' forward')\n",
    "        features = output.data.detach().numpy()[-1]\n",
    "        \n",
    "    def backward_hook(self, grad_input, grad_output):\n",
    "        nonlocal grads\n",
    "        #print('Inside ' + self.__class__.__name__ + ' backward')\n",
    "        grads = grad_output[0].detach().numpy()[-1]\n",
    "\n",
    "    \n",
    "    def learn_image( dir_path, question):\n",
    "        inputs = preprocess(dir_path, question)\n",
    "    \n",
    "        input_img = inputs[ 0 ]\n",
    "        input_img.requires_grad = True\n",
    "        grad_cam_forward_hook = vqa_resnet.resnet_layer4.r_model.layer4[2].conv3.register_forward_hook(forward_hook)\n",
    "        grad_cam_backward_hook = vqa_resnet.resnet_layer4.r_model.layer4[2].conv3.register_backward_hook(backward_hook)\n",
    "        answer = vqa_resnet(*inputs) # use model to predict the answer\n",
    "        answer_idx = np.argmax( answer.data.numpy())\n",
    "        vqa_resnet.zero_grad()\n",
    "        one_hot = np.zeros((1, answer.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][answer_idx] = 1\n",
    "        one_hot = torch.from_numpy(one_hot)\n",
    "        one_hot = torch.sum(one_hot * answer)\n",
    "        one_hot.backward( retain_graph=True)\n",
    "        grad_cam_forward_hook.remove()\n",
    "        grad_cam_backward_hook.remove()\n",
    "        \n",
    "    \n",
    "    def compute_image():\n",
    "        # features wieght based on gradiants \n",
    "        weights = np.mean( grads, axis=(1,2))\n",
    "\n",
    "        # feature importance\n",
    "        grayscale = np.zeros( features.shape[1:])\n",
    "        for (i, weight) in enumerate( weights):\n",
    "            grayscale += weight * features[ i ]\n",
    "        \n",
    "        # relu operator\n",
    "        grayscale = np.maximum( grayscale, 0)\n",
    "        \n",
    "        return grayscale\n",
    "        \n",
    "    def merge_image(image, grad_cam):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * grad_cam), cv2.COLORMAP_JET)\n",
    "        heatmap = Image.fromarray( heatmap)\n",
    "        #image_np = np.asarray(image, dtype=np.uint8)\n",
    "        return Image.blend(image, heatmap, alpha=0.7 )\n",
    "        # grad_cam_img = Image.fromarray( grad_cam * 255)\n",
    "        # background = image.copy()\n",
    "        #heatmap.paste( image ) #, heatmap )\n",
    "        #return heatmap\n",
    "        #heatmap = np.float32(heatmap) / 255\n",
    "        #cam = heatmap + image_np\n",
    "        #cam = cam / np.max(cam)\n",
    "        #return Image.fromarray(np.uint8(255 * cam))\n",
    "        #return heatmap  \n",
    "\n",
    "    learn_image( dir_path, question)\n",
    "    \n",
    "    # img value between 0 and 1\n",
    "    img = Image.open( dir_path)\n",
    "    \n",
    "    grayscale = compute_image()\n",
    "    grayscale = cv2.resize(grayscale, img.size)\n",
    "    grayscale = grayscale - np.min(grayscale)\n",
    "    grayscale = grayscale / np.max(grayscale)\n",
    "\n",
    "    \n",
    "    merged_img =  merge_image(img, grayscale)\n",
    "    return merged_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Grad_Cam('dog_cat.png', Question1)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Grad_Cam('hydrant.png', Question2)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "* The areas related to response are highlighted in the image, very clearly for first image. For second image, a second redish less visible region appears which seem less related to answer. Nonetheless, the main region of green firework appears very explictly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "TP1_Ex2_Multiclass.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLdL4qIuFqO_"
      },
      "source": [
        "## Multi-classification problem\n",
        "\n",
        "__Note:__ you might need to do\n",
        "`conda install torchvision \"pillow<7\"`\n",
        "if torchvision is not already installed on your computer, and/or for compatibility issues (the version of torchvision version supporting the last version of Pillow is not released yet)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F782b9l4FqPE"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import requests\n",
        "import os\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHg77lw7FqPF"
      },
      "source": [
        "### USPS Dataset\n",
        "* Handwritten digits with 10 classes\n",
        "* 16x16 pixels for each image \n",
        "* 6 000 data examples in training set, 1 291 examples in validation set, 2 007 in test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4IrTR_-FqPF",
        "outputId": "d186d2d6-aad5-417a-fc4a-4554d70fd6a4"
      },
      "source": [
        "url = 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "if not os.path.isdir('USPS/'):\n",
        "    os.mkdir('USPS/')\n",
        "open('USPS/usps.bz2', 'wb').write(r.content)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6579383"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXtfVVcnFqPG"
      },
      "source": [
        "# Loading USPS dataset from torchvision.dataset\n",
        "dataset = torchvision.datasets.USPS(root='USPS/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=False)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEOsCkToFqPH"
      },
      "source": [
        "# split the dataset to training and validation sets\n",
        "train_set, val_set = random_split(dataset, [6000, 1291])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "a-pFRMdvFqPH",
        "outputId": "4b628d8a-1312-496a-9662-ebabd604c833"
      },
      "source": [
        "sample_index = 88\n",
        "\n",
        "plt.imshow(dataset.data[sample_index], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "plt.title(\"image label: %d\" % dataset.targets[sample_index])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'image label: 9')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVXUlEQVR4nO3de7BdZXnH8e+PE3I5SciFIAQSCQRkQESgqcjFlCkhAgUxjjMFUQHtqG2t0Oog1FYYO9Nqtd5aB4uIpUCRIgrogFxEBh0IGiIhCYEkXIQkJzcIJCEn5Pb0j71idw7nJHu9e+19zuH9fWb2nHX2ep/9Pmft8+y19tr7Xa8iAjPLz179nYCZ9Q8Xv1mmXPxmmXLxm2XKxW+WKRe/WaZc/P1E0kJJp/Z3Hrsj6SJJv26w7VWSbkzsJznW0rn4+0lEvD0iHuzvPAYjSX8haamkjZJ+LunA/s5pMHLx26BSHC39M3AuMB54Dri5P3MarFz8/UTS85JmFMtXSbpV0o2SNkiaL+ltkq6QtFrSi5Jm1sVeLGlR0fZZSZ/s8diXSeqStKLYS4akw4p1wyR9TdILklZJ+q6kEQ3m/K0il/WSHpP0nh5Nhku6pchrrqR31sUeKOk2SWskPSfpM4mb7mzg1ohYGBFbgH8Cpkuamvh42XLxDxznADcA44DfAfdQe34OAr4E/Gdd29XUimAf4GLgG5KOB5B0BvB3wAzgMODUHv18GXgbcGyx/iDgiw3m+NsibjzwP8CtkobXrT8XuLVu/e2S9pa0F/BTYF7R32nApZLe21snkp6Q9KHd5KFelo9u8G+wnSLCt364Ac8DM4rlq4D76tadA2wEOorfRwMBjO3jsW4HLimWrwP+pW7dYUXsYdQK5TVgat36E4Hn+njci4Bf7+ZvWAe8s+5vmF23bi+gC3gPcALwQo/YK4Af1MXe2OB2mwGsBY4BRlB7UdwBnN/fz+lguw1p/uXDKrKqbrkbWBsR2+t+BxgFvCLpTOBKanvwvYBOYH7R5kBgTt1jvVi3vF/R9jHpDztPAR2NJCjpc8DHiz6C2pHHhN76iogdkpbVtT1Q0it1bTuAXzXSb72IuF/SlcBtRf/fBDYAy8o+Vu5c/IOMpGHU/vE/CtwREVsl3c7/H/52AZPqQibXLa+l9kLy9ohYXrLf9wCXUTtkX1gU9zp2PQSfXNd+ryKPFcA2akcXh5fpsy8R8R3gO0U/bwP+AVhQxWPnxO/5B5+hwDBgDbCtOAqYWbf+f4GLJR0pqRP4x50rImIH8D1q5wjeAiDpoL7ee/cwmloRrwGGSPoitT1vvT+S9AFJQ4BLgdeB2cBvgA2SPi9phKQOSUdL+uOyf7yk4UWsJL0VuAb4VkSsK/tYuXPxDzIRsQH4DLUiXwd8CLizbv3dwLeBXwJLqRUf1AoR4PM775e0HrgfOKKBru8Bfg4sBn4PbGbXtxQAdwB/XuT1EeADEbG1ePtyNrWThc9ROwK5FhjTW0fFF6Au6COP4dROJm6k9qLyCHUvcNY4FSdR7E1K0pHUDomHRcS2/s7HBg7v+d+EJM0qPs8fB3wF+KkL33py8b85fZLadwGeAbYDf9m/6dhA5MN+s0x5z2+WqbZ+zj9hwoSYMmVKO7sc0Hbs2JEUt3Xr1tIxW7ZsSepr8+bNSXGbNm0qHfP666/vuVEvtm/fvudGFenoaOj7UG8watSo0jFjx44tHbNixQpeeeUV7bllm4t/ypQpzJkzZ88NB5nUf76UAgFYuXJl6ZgXX+z5qVxjlixZkhQ3d+7ctvW1YcOGpLgUKQUJcOKJJ5aOmTVrVumYCy7o6xPSN/Jhv1mmXPxmmWqq+CWdIenp4qoql1eVlJm1XnLxS+qgNrjiTOAo4HxJR1WVmJm1VjN7/ncBSyPi2ahdUeWH1C7mYGaDQDPFfxC7DuxYVty3C0mfkDRH0pw1a9Y00Z2ZVanlJ/wi4pqImBYR0/bbb79Wd2dmDWqm+Jez64UiJhX3mdkg0Ezx/xY4XNIhkoYC51E3rtzMBrbkb/hFxDZJn6Z2kYcO4LqIWFhZZmbWUk19vTci7gLuqigXM2sjf8PPLFNv2qv3pl6nIGX026pVq/bcqBcLFqRdcPZXvyp9xWsWLVqU1Nf69euT4rq7u/fcqIeNGzcm9ZUysGf16tVJfaWOPEwZjDVhwoQ9N+qhzDb0nt8sUy5+s0y5+M0y5eI3y5SL3yxTLn6zTLn4zTLl4jfLlIvfLFMufrNMufjNMuXiN8vUoBjYkzKtVeogkcWLF5eOefjhh5P6mj17dlJcyrUQR48endTXCSeckBSXMi3biBEjkvpKGaTzwAMPJPX14IMPJsW98MILpWNSBmOVmV7Ne36zTLn4zTLl4jfLVDMz9kyW9EtJT0paKOmSKhMzs9Zq5oTfNuCzETFX0mjgMUn3RcSTFeVmZi2UvOePiK6ImFssbwAW0cuMPWY2MFXynl/SFOA44NFe1nm6LrMBqOnilzQKuA24NCLecLVHT9dlNjA1VfyS9qZW+DdFxI+rScnM2qGZs/0Cvg8sioivV5eSmbVDM3v+k4GPAH8q6fHidlZFeZlZizUzV9+vAVWYi5m1kb/hZ5apto/qSxmht27dutIxc+bMKR0DcM8995SOWbp0aVJfKdMxAZxzzjmlY4466qikvg4++OCkuLFjx5aO6ejoSOor5f9j2LBhSX2lPtebNm0qHbPvvvuWjimzDb3nN8uUi98sUy5+s0y5+M0y5eI3y5SL3yxTLn6zTLn4zTLl4jfLlIvfLFMufrNMufjNMtXWgT3bt2/n1VdfLR33yCOPlI659dZbS8cArFy5snTMsccem9TXjBkzkuJSBumMHz8+qa+hQ4cmxaUoM9VUve7u7tIxqdO5dXZ2JsUdccQRpWNOPvnk0jG33HJLw2295zfLlIvfLFMufrNMVXHp7g5Jv5P0syoSMrP2qGLPfwm12XrMbBBp9rr9k4A/A66tJh0za5dm9/zfBC4Dyl+Yz8z6VTOTdpwNrI6Ix/bQ7g9z9a1duza1OzOrWLOTdrxP0vPAD6lN3nFjz0b1c/WlXq3WzKrXzBTdV0TEpIiYApwHPBARH64sMzNrKX/Ob5apSr7bHxEPAg9W8Vhm1h7e85tlqq2j+rq7u5k/f37puDIjlXZ68sknS8cAzJw5s3TMrFmzkvo68sgjk+JGjhyZFJfi9ddfT4pbtWpV6Zhnn302qa9HH320LTEAhx56aFLcmWeeWTrmmGOOKR1TZtSh9/xmmXLxm2XKxW+WKRe/WaZc/GaZcvGbZcrFb5YpF79Zplz8Zply8ZtlysVvlikXv1mmXPxmmWrrqL4NGzbw0EMPlY57+OGHS8eMGjWqdAzAO97xjtIxU6dOTeprxIgRSXERUTpm06ZNSX0tXLgwKe7+++8vHTNnzpykvtavX1865oADDkjqa/r06Ulxp5xySumYfffdt3TMkCGNl7T3/GaZcvGbZcrFb5apZmfsGSvpR5KekrRI0olVJWZmrdXsCb9vAT+PiA9KGgo0fg0hM+tXycUvaQwwHbgIICK2AFuqScvMWq2Zw/5DgDXAD4opuq+V9IYrS9ZP1/Xaa6810Z2ZVamZ4h8CHA9cHRHHAa8Bl/dsVD9dVzuvOmtmu9dM8S8DlkXEzmsg/4jai4GZDQLNzNW3EnhR0hHFXacBaRfLN7O2a/Zs/98ANxVn+p8FLm4+JTNrh6aKPyIeB6ZVlIuZtVFbB/Zs3ryZp59+unRcV1dX6ZiUQREAS5cuLR0zadKkpL7Gjx+fFLdt27bSMcuWLUvq66677kqKSxnYs3bt2qS+Jk+eXDom9f9jxYoVSXHLly8vHTNx4sTSMTt27Gi4rb/ea5YpF79Zplz8Zply8ZtlysVvlikXv1mmXPxmmXLxm2XKxW+WKRe/WaZc/GaZcvGbZcrFb5apto7qk8Tee+9dOm6vvcq/RqWOYrvhhhtKx8yePTupr9QpozZv3lw6JnU0WsooTICVK1cmxaXYvn176ZjU7TF//vykuO7u7tIxb33rW0vHbN26teG23vObZcrFb5YpF79ZppqdrutvJS2UtEDSzZKGV5WYmbVWcvFLOgj4DDAtIo4GOoDzqkrMzFqr2cP+IcAISUOozdOXdgrVzNqumev2Lwe+BrwAdAGvRsS9PdvVT9eV8hGVmbVGM4f944Bzqc3ZdyAwUtKHe7arn65r+HCfEjAbKJo57J8BPBcRayJiK/Bj4KRq0jKzVmum+F8A3i2pU5KoTde1qJq0zKzVmnnP/yi1yTnnAvOLx7qmorzMrMWana7rSuDKinIxszbyN/zMMtXWUX2dnZ0cd9xxpeMWLlxYOmbx4sWlYwBWr15dOmbjxo1Jfe2zzz5JcSkjI8vM4VYvdU67lBGLqZ8GpYyYe+qpp5L6eu2115LiVq1aVTpmy5YtpWMiouG23vObZcrFb5YpF79Zplz8Zply8ZtlysVvlikXv1mmXPxmmXLxm2XKxW+WKRe/WaZc/GaZauvAnn322YfTTz+9dFxnZ2fpmCeeeKJ0DMCGDRtKx9SuZVLeqFGjkuJSBs2MGTMmqa8JEyYkxY0dO7Z0TOp2nDdvXumYq6++Oqmv9evXJ8WlbMeUgU5lprbznt8sUy5+s0y5+M0ytcfil3SdpNWSFtTdN17SfZKWFD/HtTZNM6taI3v+/wLO6HHf5cAvIuJw4BfF72Y2iOyx+CPiIeDlHnefC1xfLF8PvL/ivMysxVLf8+8fEV3F8kpg/74a1k/XtW7dusTuzKxqTZ/wi9oVA/u8amD9dF3jxvnUgNlAkVr8qyRNBCh+lr/krZn1q9TivxO4sFi+ELijmnTMrF0a+ajvZuAR4AhJyyR9HPgycLqkJdQm7Pxya9M0s6rt8bv9EXF+H6tOqzgXM2sjf8PPLFNtHdU3bNgwpk6dWjouZcqo6dOnl44B2Lp1a1JcipRptwBGjBhROmbYsGFt6wvS/rbUac9eeuml0jGpf1eZ6bDqHXLIIaVjRo8eXTrGo/rMbI9c/GaZcvGbZcrFb5YpF79Zplz8Zply8ZtlysVvlikXv1mmXPxmmXLxm2XKxW+WqbYO7JGUNOAjZaqjlMFAg0XqtFbt7CtlAMy2bduS+lqzZk3pmO7u7qS+UgbbAEycOLF0zMiRI0vHdHR0NNzWe36zTLn4zTLl4jfLVOp0XV+V9JSkJyT9RFL5+ZjNrF+lTtd1H3B0RBwDLAauqDgvM2uxpOm6IuLeiNh5anY2MKkFuZlZC1Xxnv9jwN19rayfrivlIxkza42mil/SF4BtwE19tamfrmu//fZrpjszq1Dyl3wkXQScDZwWqZc0NbN+k1T8ks4ALgP+JCI2VZuSmbVD6nRd/wGMBu6T9Lik77Y4TzOrWOp0Xd9vQS5m1kb+hp9Zpto6qi9Vysiydo58s2ps2pR2+qirq6t0TOqovvHjxyfFDR8+vHRMmam3UnjPb5YpF79Zplz8Zply8ZtlysVvlikXv1mmXPxmmXLxm2XKxW+WKRe/WaZc/GaZcvGbZcrFb5apQTGqz/KQMo8jQGdnZ+mYoUOHJvW1efPmpLiXX355z40q6GvHjh0Nt/We3yxTLn6zTCVN11W37rOSQlL5ObTNrF+lTteFpMnATOCFinMyszZImq6r8A1ql+/2NfvNBqGk9/ySzgWWR8S8Btp6ui6zAah08UvqBP4e+GIj7T1dl9nAlLLnnwocAsyT9Dy1GXrnSjqgysTMrLVKf8knIuYDb9n5e/ECMC0i1laYl5m1WOp0XWY2yKVO11W/fkpl2ZhZ2/gbfmaZ8sAea4mU6dLGjBmT1Nexxx5bOuakk05K6isi7Wsto0aNKh3T6mnqvOc3y5SL3yxTLn6zTLn4zTLl4jfLlIvfLFMufrNMufjNMuXiN8uUi98sUy5+s0y5+M0y5eI3y5RSRykldSatAX7fx+oJwEC4GpDz2JXz2NVAz+PgiGjoYpltLf7dkTQnIqY5D+fhPNqThw/7zTLl4jfL1EAq/mv6O4GC89iV89jVmyaPAfOe38zaayDt+c2sjVz8Zplqa/FLOkPS05KWSrq8l/XDJN1SrH9U0pQW5DBZ0i8lPSlpoaRLemlzqqRXJT1e3BqalzAxn+clzS/6mdPLekn6drFNnpB0fMX9H1H3dz4uab2kS3u0adn2kHSdpNWSFtTdN17SfZKWFD/H9RF7YdFmiaQLW5DHVyU9VWz3n0ga20fsbp/DCvK4StLyuu1/Vh+xu62vN4iIttyADuAZ4FBgKDAPOKpHm78Cvlssnwfc0oI8JgLHF8ujgcW95HEq8LM2bZfngQm7WX8WcDcg4N3Aoy1+jlZS+6JIW7YHMB04HlhQd9+/ApcXy5cDX+klbjzwbPFzXLE8ruI8ZgJDiuWv9JZHI89hBXlcBXyugedut/XV89bOPf+7gKUR8WxEbAF+CJzbo825wPXF8o+A05Ry8fLdiIiuiJhbLG8AFgEHVdlHxc4F/jtqZgNjJU1sUV+nAc9ERF/fwqxcRDwEvNzj7vr/g+uB9/cS+l7gvoh4OSLWAfcBZ1SZR0TcGxHbil9nU5uUtqX62B6NaKS+dtHO4j8IeLHu92W8sej+0KbY6K8C+7YqoeJtxXHAo72sPlHSPEl3S3p7q3IAArhX0mOSPtHL+ka2W1XOA27uY127tgfA/hHRVSyvBPbvpU07twvAx6gdgfVmT89hFT5dvP24ro+3QaW3R7Yn/CSNAm4DLo2I9T1Wz6V26PtO4N+B21uYyikRcTxwJvDXkqa3sK8+SRoKvA+4tZfV7dweu4jaMW2/fh4t6QvANuCmPpq0+jm8GpgKHAt0Af9WxYO2s/iXA5Prfp9U3NdrG0lDgDHAS1UnImlvaoV/U0T8uOf6iFgfERuL5buAvSVNqDqP4vGXFz9XAz+hdvhWr5HtVoUzgbkRsaqXHNu2PQqrdr61KX6u7qVNW7aLpIuAs4ELiheiN2jgOWxKRKyKiO0RsQP4Xh+PX3p7tLP4fwscLumQYi9zHnBnjzZ3AjvP2n4QeKCvDZ6qOIfwfWBRRHy9jzYH7DzXIOld1LZTK16ERkoavXOZ2gmmBT2a3Ql8tDjr/27g1bpD4iqdTx+H/O3aHnXq/w8uBO7opc09wExJ44rD4JnFfZWRdAZwGfC+iNjUR5tGnsNm86g/xzOrj8dvpL52VcUZyhJnMs+idnb9GeALxX1forZxAYZTO+xcCvwGOLQFOZxC7TDyCeDx4nYW8CngU0WbTwMLqZ0xnQ2c1KLtcWjRx7yiv53bpD4XAd8pttl8YFoL8hhJrZjH1N3Xlu1B7QWnC9hK7X3qx6md5/kFsAS4HxhftJ0GXFsX+7Hif2UpcHEL8lhK7X30zv+TnZ9EHQjctbvnsOI8biie+yeoFfTEnnn0VV+7u/nrvWaZyvaEn1nuXPxmmXLxm2XKxW+WKRe/WaZc/GaZcvGbZer/ABKuoZwMq6BaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neIQ4OAMFqPH"
      },
      "source": [
        "### Training the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwM46oyiFqPH"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        self.c1 = nn.Conv2d(1,6, 5) # Convolution kernel 6 layer size 5, stride 1\n",
        "        self.c2 = nn.Conv2d(6,16, 5) # Convolution kernel 16 layer size 5, stride 1\n",
        "        #self.c3 = nn.Conv2d(16,32, 5) # Convolution kernel 16 layer size 5, stride 1\n",
        "        # self.l1 = nn.Linear(16 * 12 * 12, 120)\n",
        "        self.l1 = nn.Linear(16 * 8 * 8, 10)\n",
        "        self.l2 = nn.Linear(84, 10)\n",
        "        #self.l3 = nn.Linear(84, 10)\n",
        "        #self.l3 = nn.Linear(84, 10)\n",
        "        #self.l3 = nn.Linear(84, 10)\n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        h1 = F.relu(self.c1(inputs))\n",
        "        h2 = F.relu(self.c2(h1))\n",
        "        #h2 = F.relu(self.c3(h2))\n",
        "        #print(h1.shape)\n",
        "        #h3 = F.relu(self.l1(h2.view(h2.shape[0],-1)))\n",
        "        #h3 = F.relu(self.l1(h1.view(h1.shape[0],-1)))\n",
        "        h3 = h2.view(h2.shape[0],-1)\n",
        "        h4 = self.l1( h3 )\n",
        "        #h4 = F.relu(self.l1(h3))\n",
        "        #h5 = F.relu(self.l2(h4))\n",
        "        #outputs = F.softmax(self.l3(h5), dim=1)# Use softmax as the activation function for the last layer\n",
        "        #outputs = F.softmax(self.l4(h5), dim=1)# Use softmax as the activation function for the last layer\n",
        "        #outputs = F.softmax(self.l1(h3), dim=1)# Use softmax as the activation function for the last layer\n",
        "        #outputs = self.l2(h4)\n",
        "        outputs = h4\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXnnqDG_FqPI"
      },
      "source": [
        "# Create the model: \n",
        "model = Model()\n",
        "\n",
        "# Choose the hyperparameters for training: \n",
        "num_epochs = 40\n",
        "batch_size = 20\n",
        "\n",
        "# Use mean squared loss function \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use SGD optimizer with a learning rate of 0.01\n",
        "# It is initialized on our model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z2afEdjFqPI"
      },
      "source": [
        "# define a function for training\n",
        "def train(num_epochs, batch_size, criterion, optimizer, model, dataset):\n",
        "    train_error = []\n",
        "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_average_loss = 0.0\n",
        "        for (images, labels) in train_loader:\n",
        "            #y_pre = model(images.view(batch_size, -1))\n",
        "            y_pre = model( images)\n",
        "            #print( labels.shape)\n",
        "            #reshape the inputs from [N, img_shape, img_shape] to [N, img_shape*img_shape] \n",
        "            \n",
        "            # One-hot encoding or labels so as to calculate MSE error:\n",
        "            #labels_one_hot = torch.FloatTensor(batch_size, 10)\n",
        "            #labels_one_hot.zero_()\n",
        "            #labels_one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
        "           \n",
        "            \n",
        "            loss = criterion(y_pre, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
        "        train_error.append(epoch_average_loss)\n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, epoch_average_loss))\n",
        "    return train_error"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvoKfUOLFqPI",
        "outputId": "2790bb76-f90e-4fa4-a183-c01b41cc7c67"
      },
      "source": [
        "train_error = train(num_epochs, batch_size, criterion, optimizer, model, train_set)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/40], Loss: 0.5991\n",
            "Epoch [2/40], Loss: 0.2323\n",
            "Epoch [3/40], Loss: 0.1563\n",
            "Epoch [4/40], Loss: 0.1209\n",
            "Epoch [5/40], Loss: 0.0973\n",
            "Epoch [6/40], Loss: 0.0740\n",
            "Epoch [7/40], Loss: 0.0674\n",
            "Epoch [8/40], Loss: 0.0633\n",
            "Epoch [9/40], Loss: 0.0501\n",
            "Epoch [10/40], Loss: 0.0426\n",
            "Epoch [11/40], Loss: 0.0410\n",
            "Epoch [12/40], Loss: 0.0321\n",
            "Epoch [13/40], Loss: 0.0240\n",
            "Epoch [14/40], Loss: 0.0228\n",
            "Epoch [15/40], Loss: 0.0306\n",
            "Epoch [16/40], Loss: 0.0202\n",
            "Epoch [17/40], Loss: 0.0169\n",
            "Epoch [18/40], Loss: 0.0117\n",
            "Epoch [19/40], Loss: 0.0141\n",
            "Epoch [20/40], Loss: 0.0125\n",
            "Epoch [21/40], Loss: 0.0138\n",
            "Epoch [22/40], Loss: 0.0217\n",
            "Epoch [23/40], Loss: 0.0134\n",
            "Epoch [24/40], Loss: 0.0132\n",
            "Epoch [25/40], Loss: 0.0077\n",
            "Epoch [26/40], Loss: 0.0101\n",
            "Epoch [27/40], Loss: 0.0161\n",
            "Epoch [28/40], Loss: 0.0049\n",
            "Epoch [29/40], Loss: 0.0043\n",
            "Epoch [30/40], Loss: 0.0038\n",
            "Epoch [31/40], Loss: 0.0023\n",
            "Epoch [32/40], Loss: 0.0033\n",
            "Epoch [33/40], Loss: 0.0039\n",
            "Epoch [34/40], Loss: 0.0361\n",
            "Epoch [35/40], Loss: 0.0069\n",
            "Epoch [36/40], Loss: 0.0035\n",
            "Epoch [37/40], Loss: 0.0031\n",
            "Epoch [38/40], Loss: 0.0029\n",
            "Epoch [39/40], Loss: 0.0026\n",
            "Epoch [40/40], Loss: 0.0214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s63_gSkFqPJ"
      },
      "source": [
        "# plot the training error wrt. the number of epochs: \n",
        "plt.plot(range(1, num_epochs+1), train_error)\n",
        "plt.xlabel(\"num_epochs\")\n",
        "plt.ylabel(\"Train error\")\n",
        "plt.title(\"Visualization of convergence\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5E_A0kUFqPJ"
      },
      "source": [
        "### Evaluate the Model on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWX3JqueFqPJ"
      },
      "source": [
        "# Calculate the accuracy to evaluate the model\n",
        "def accuracy(dataset, model):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        dataloader = DataLoader(dataset)\n",
        "        for images, labels in dataloader:\n",
        "            #images = images.view(-1, 16*16)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1) \n",
        "            correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Accuracy of the model : {:.2f} %'.format(100*correct.item()/ len(dataset)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reJ3JL8PFqPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d4b4c9-2f64-4a3d-b0bc-7d9becb82c02"
      },
      "source": [
        "accuracy(val_set, model)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model : 97.68 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "l1LRhRd3FqPK",
        "outputId": "7950473e-9bf0-4475-ee08-8c02c7d2e8aa"
      },
      "source": [
        "val_index = 66\n",
        "\n",
        "(image, label) = val_set[val_index]\n",
        "print(image.unsqueeze(1).shape)\n",
        "output = model( image.unsqueeze(1) )\n",
        "\n",
        "#output = model(image.view(-1, 16*16))\n",
        "_, prediction = torch.max(output.data, 1)\n",
        "\n",
        "plt.imshow(image.view(16, 16), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "plt.title(\"Prediction label: %d\" % prediction)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 16, 16])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Prediction label: 3')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUtkA28uFqPK"
      },
      "source": [
        "### Exercise 1: Impact of the architecture of the model\n",
        "Define your own class `Model` to improve the predictions:\n",
        "\n",
        "* The convolutional layer can be a good choice to deal with images. Replace nn.Linear with [nn.Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d).\n",
        "* Try to add more layers (1, 2, 3, more ?)\n",
        "* Change the number of neurons in hidden layers (5, 10, 20, more ?)\n",
        "* Try different activation functions such as [sigmoid](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid), [tanh](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh), [relu](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu), etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbNt2gKoFqPK"
      },
      "source": [
        "### Exercise 2: Impact of the optimizer\n",
        "Retrain the model by using different parameters of the optimizer; you can change its parameters in the cell initializing it, after the definition of your model.\n",
        "\n",
        "* Use different batch sizes, from 10 to 1 000 for instance\n",
        "* Try different values of the learning rate (between 0.001 and 10), and see how these impact the training process. Do all network architectures react the same way to different learning rates?\n",
        "* Change the duration of the training by increasing the number of epochs\n",
        "* Try other optimizers, such as [Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) or [RMSprop](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jyriq7bFqPL"
      },
      "source": [
        "### Exercise 3: Impact of the loss function\n",
        "The MSE error is rarely used in this case. The cross entropy loss can be a better choice for multi-classification problems. In pytorch, the cross entropy loss is defined by [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#crossentropyloss). Replace the MSE loss by this one to observe its impact.\n",
        "\n",
        "**Note:** In order to use nn.CrossEntropyLoss correctly, don't add an activation function to the last layer of your network. And one-hot encoding is no longer needed to calculate the loss, delete the encoding procedures in function `train`.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Ad1pt9FqPL"
      },
      "source": [
        "### Exercise 4: Prediction on test set\n",
        "\n",
        "Once you have a model that seems satisfying on the validation dataset, you SHOULD evaluate it on a test dataset that has never been used before, to obtain a final accuracy value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f2-27WuFqPL",
        "outputId": "62af03aa-952e-4c8d-efbb-28d0dc3a054d"
      },
      "source": [
        "url = 'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('USPS/usps.t.bz2', 'wb').write(r.content)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1831726"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9wCPWKJFqPL"
      },
      "source": [
        "# Loading MNIST test set from torchvision.dataset\n",
        "test_set = torchvision.datasets.USPS(root='USPS/',\n",
        "                                         train=False,\n",
        "                                         transform=transforms.ToTensor(),\n",
        "                                         download=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-N948KuFqPM",
        "outputId": "cf4ceffe-f17e-4344-cc5c-5d8fdfc4c90a"
      },
      "source": [
        "accuracy(test_set, model)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model : 95.37 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRibv_mdFqPM",
        "outputId": "3943e70a-e85e-4dc9-cfd3-0c353b36eef4"
      },
      "source": [
        "model"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (c1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (c2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (l1): Linear(in_features=1024, out_features=10, bias=True)\n",
              "  (l2): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8JQmb2KGlLD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}